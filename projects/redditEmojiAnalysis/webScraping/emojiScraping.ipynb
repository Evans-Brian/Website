{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "dataisbeautiful finished\n",
      "\n",
      "--- 89.21494483947754 seconds ---\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datetime import datetime as dt\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import emoji\n",
    "import re\n",
    "import urllib.request\n",
    "from emoji.unicode_codes import UNICODE_EMOJI\n",
    "from collections import Counter\n",
    "import timeit\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"hello\")\n",
    "    \n",
    "def extract_emojis(a_string):\n",
    "    \"\"\"Finds all emojis in a string and converts them to unicode\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a_string: str\n",
    "        string that is searched for emojis\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    emojiTextArray: list\n",
    "        contains unicode for each emoji found in a_string\n",
    "    \"\"\"\n",
    "    emojiTextArray = []\n",
    "    for c in a_string:\n",
    "        if c in emoji.UNICODE_EMOJI:\n",
    "            emojiTextArray.append(UNICODE_EMOJI[c])\n",
    "    return emojiTextArray\n",
    "\n",
    "def extract_profanity(a_string):\n",
    "    \"\"\"Finds all emojis in a string and converts them to unicode\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a_string: str\n",
    "        string that is searched for profanity\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    profanityArray: list\n",
    "        contains counts for each word found in swearWords\n",
    "    \"\"\"\n",
    "    #convert string to lower for consistency\n",
    "    a_string = a_string.lower()\n",
    "    #list of profanity to search string for\n",
    "    swearWords = ['fuck', 'shit', 'bitch', 'dick']\n",
    "    profanityArray = []\n",
    "    for swear in swearWords:\n",
    "        count = a_string.count(swear)\n",
    "        profanityArray.append(count)\n",
    "    return profanityArray\n",
    "\n",
    "\n",
    "def extract_commentData(comments, limit, subreddit):\n",
    "    \"\"\"Extracts comment data from a subreddit containing score, number of emojis, individual and sum profanity counts, total emoji count, and total comments searched\n",
    "    Ensures even ratio of comments containing emojis, profanity, and neither. Receives data from PushShift query extract_all_commentData (max 1000 comments).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    comments: list\n",
    "        list containing a dictionary for each comment with all data returned by PushShift relating to that comment\n",
    "    limit: int\n",
    "        number of comments to return data on\n",
    "    subreddit: string\n",
    "        subreddit the comments originated from\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    commentData: list\n",
    "        list of lists containing individual comment score, emoji count, individual profanity count, and total profanity count\n",
    "    lastTime: int\n",
    "        unix time that last comment read was posted\n",
    "    totalEmojis: list\n",
    "        list containing all unicode of all emojis found in all comments (max 1000 at a time) queried\n",
    "    totalProfanity: list\n",
    "        list of total specific profanity counts in all comments (max 1000 at a time) queried\n",
    "    commentCount: int\n",
    "        total comments queried (max 1000). Will likely be larger than limit, unless subreddit has a very high frequency of emojis\n",
    "    \"\"\"\n",
    "    # initialize storage\n",
    "    commentData = []\n",
    "    emojiCount = 0\n",
    "    profanityCount = 0\n",
    "    normalCount = 0\n",
    "    commentCount = 0\n",
    "    totalEmojis = []\n",
    "    totalProfanity = []    \n",
    "    \n",
    "    # searches each comment in comments\n",
    "    for comment in comments:\n",
    "        # ends if comments to be returned equal or surpass the limit\n",
    "        if len(commentData) >= limit: break\n",
    "\n",
    "        individualCommentData = []\n",
    "        # extracts emojis unicode for each comment\n",
    "        emojis = extract_emojis(comment['body'])\n",
    "        \n",
    "        # adds all unicode to a comprehensive list\n",
    "        totalEmojis += emojis          \n",
    "        \n",
    "        # extracts proganity counts from comment\n",
    "        profanityArray = extract_profanity(comment['body'])\n",
    "        totalProfanity.append(profanityArray)\n",
    "        \n",
    "        #appends comment score, total emojis, specific and total profanity counts, and subreddit to individualCommentData\n",
    "        individualCommentData.append(comment['score'])\n",
    "        individualCommentData.append(len(emojis))\n",
    "        individualCommentData.append(sum(profanityArray))\n",
    "        individualCommentData.append(len(comment['body'].split()))\n",
    "        individualCommentData.append(subreddit)\n",
    "        \n",
    "        # appends comments with emojis to the overall comment data. Increments emoji comment count\n",
    "        if len(emojis) > 0:\n",
    "            commentData.append(individualCommentData)\n",
    "            emojiCount += 1\n",
    "            \n",
    "        # appends comments with profanity to the overall comment data if there are less comments with profanity than emojis\n",
    "        # Increments profanity comment count\n",
    "        if len(emojis) == 0 and sum(profanityArray) > 0 and profanityCount < emojiCount:\n",
    "            commentData.append(individualCommentData)\n",
    "            profanityCount += 1\n",
    "            \n",
    "        # appends comments without emojis or profanity to the overall comment data if there are less comments without emojis and profanity\n",
    "        # Increments profanity comment count       \n",
    "        if len(emojis) == 0 and normalCount < emojiCount:\n",
    "            commentData.append(individualCommentData)\n",
    "            normalCount += 1\n",
    "        \n",
    "        #increments comment count to summarize total number of comments queried to reach limit \n",
    "        commentCount += 1\n",
    "    \n",
    "    lastTime = comment['created_utc']\n",
    "    #sums all profanity for all comments queried\n",
    "    totalProfanity = [sum(i) for i in zip(*totalProfanity)]\n",
    "    \n",
    "    return commentData, lastTime, totalEmojis, totalProfanity, commentCount\n",
    "\n",
    "def extract_all_commentData(after, before, subreddit, limit):\n",
    "    \"\"\"Calls extract_commentData on comments queried from PushShift. PushShift has a query limit of 1000 comments. extract_all_commentData will\n",
    "    continue to call extract_commentData untul the desired number of comments is reached.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    after: int\n",
    "        starting time to query comments from\n",
    "    before: int\n",
    "        ending time to query comments form\n",
    "    subreddit: string\n",
    "        subreddit to pull comments from\n",
    "    limit:int\n",
    "        limit of comments to retrieve and save\n",
    "    Returns\n",
    "    -------\n",
    "    commentData: list\n",
    "        list of lists containing individual comment score, emoji count, individual profanity count, and total profanity count\n",
    "    lastTime: int\n",
    "        unix time that last comment read was posted\n",
    "    totalEmojis: list\n",
    "        list containing all unicode of all emojis found in all comments (no max) queried\n",
    "    totalProfanity: list\n",
    "        list of total specific profanity counts in all comments (no max) queried\n",
    "    commentCount: int\n",
    "        total comments queried. Will likely be larger than limit, unless subreddit has a very high frequency of emojis\n",
    "    \"\"\"\n",
    "    #first line of csv\n",
    "    allCommentData = [['score', 'emojiCount', 'profanityCount', 'wordCount', 'subreddit']]\n",
    "    # initialize storage   \n",
    "    totalCommentCount = 0\n",
    "    totalEmojis = []\n",
    "    totalProfanity = [0, 0, 0, 0]\n",
    "\n",
    "    #PushShift query. Pulls 1000 comments at a time \n",
    "    while True:\n",
    "        with urllib.request.urlopen(\"https://api.pushshift.io/reddit/comment/search/?subreddit=\" + subreddit + \"&size=1000&after=\" + str(after) + \"&before=\" + str(before)) as url:\n",
    "            pushShiftData = json.loads(url.read().decode())\n",
    "        #breaks loops if limit is reached or query is empty\n",
    "        if limit <= 0 or len(pushShiftData[\"data\"]) <= 0: break\n",
    "            \n",
    "        commentData, after, emojis, profanity, commentCount= extract_commentData(pushShiftData[\"data\"], limit, subreddit)\n",
    "        limit -= len(commentData)\n",
    "        allCommentData += commentData\n",
    "        totalEmojis += emojis\n",
    "        totalProfanity = [totalProfanity[i]+profanity[i] for i in range(len(profanity))]\n",
    "        totalCommentCount += commentCount\n",
    "\n",
    "    return allCommentData, totalCommentCount, totalEmojis, totalProfanity\n",
    "\n",
    "def writeData(after, before, subredditArray, limit):\n",
    "    \"\"\"Calls extract_all_commentData on subreddits specified and writes comment data to CSV. Total profanity count, total emoji count, and total comments \n",
    "    for all subreddits are written to 1 JSON.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    after: int\n",
    "        starting time to query comments from. Passed to extract_all_commentData\n",
    "    before: int\n",
    "        ending time to query comments form. Passed to extract_all_commentData\n",
    "    subreddit: string\n",
    "        subreddit to pull comments from. Passed to extract_all_commentData\n",
    "    limit:int\n",
    "        limit of comments to retrieve and save. Passed to extract_all_commentData\n",
    "    \"\"\"\n",
    "    jsonData = []\n",
    "    #loops through subreddit array and extracts data\n",
    "    for subreddit in subredditArray:  \n",
    "        allCommentData, commentCount, allEmojis, allProfanity = extract_all_commentData(after, before, subreddit, limit)\n",
    "        # writes comment data to csvs named after the subreddit they came from\n",
    "        \n",
    "        if len(subreddit) == 0:\n",
    "            subreddit = 'all'\n",
    "        \n",
    "        with open('../analysis/data/' + subreddit + '.csv', 'w') as outcsv:   \n",
    "            #configure writer to write standard csv file\n",
    "            writer = csv.writer(outcsv, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "            for commentData in allCommentData:\n",
    "                #Write item to outcsv\n",
    "                writer.writerow(commentData)\n",
    "\n",
    "        # counts 3 most common emojis in each subreddit\n",
    "        emojis_to_count = (emoji for emoji in allEmojis)\n",
    "        emojiCounter = Counter(emojis_to_count)\n",
    "        emojiTop3 = dict(emojiCounter.most_common(3))\n",
    "        \n",
    "        # counts specific profanity in each subreddit\n",
    "        swearWords = ['f---', 's---', 'b----', 'd---']\n",
    "        profanityCounter = []\n",
    "        for index, swear in enumerate(swearWords):\n",
    "            profanityCounter.append((swear, allProfanity[index]))\n",
    "        # adds emoji and profanity counts to dict\n",
    "        dic = dict(profanityCounter, **emojiTop3)\n",
    "        subredditData = dict({'totalComments':commentCount, 'emojiCount': len(allEmojis), 'profanityCount': sum(allProfanity)}, **dic)\n",
    "        subredditData['subreddit'] = subreddit\n",
    "        # appends subreddit name to dict\n",
    "        jsonData.append(subredditData)\n",
    "        \n",
    "        print(subreddit + \" finished\")\n",
    "        \n",
    "    # writes emoji and profanity counts and subreddit name to JSON\n",
    "    with open('../analysis/data/countData.json', 'w') as outjson:\n",
    "        json.dump(jsonData , outjson)\n",
    "        \n",
    "        \n",
    "#########################################################################################################################\n",
    "\n",
    "# subreddits = ['funny', 'changemyview', 'dataisbeautiful', 'nba', 'emojipasta']\n",
    "subreddits = ['dataisbeautiful']\n",
    "writeData('2019-04-27', '2019-06-28', subreddits, 1500)\n",
    "\n",
    "print(\"\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\")\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
